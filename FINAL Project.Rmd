---
title: "mental_health final code"
output: html_notebook
---

#import libraries
```{r}
  library(ggplot2)
  library(gridExtra)
  library(Hmisc)
  library(reshape2)
  library(DMwR)
  library(caret)
  library(pROC)
  library(RWeka)
  library(corrplot)
  library(mice)
  library(VIM)
  library(lattice)
  library(plotROC)
  library(pROC)
  library(ipred)
  library(e1071)
```
#load database which is in .xpt format.
```{r}
  readfile <- function()
  { 
  n <- readline(prompt="Enter the file path for the BRFSS data: ")
  return(n)
  }
  
  mydata=sasxport.get((readfile()))
```

#data preprocessing
```{r}
clean_data = mydata
names(clean_data)=(contents(mydata))$contents$Labels


#remove record indentification
clean_data1=clean_data[,-c(1:26)]



#count the no. of na(S)
na_count <-sapply(clean_data1, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count
```
#replacing options with ans NO as NA
```{r}
clean_data2=as.data.frame(clean_data1)
clean_data2[] <- lapply(clean_data1[], as.numeric)
clean_data2[clean_data2 == 88] <- NA
clean_data2[clean_data2 == 7] <- NA
clean_data2[clean_data2 == 9] <- NA
```
#counting the number of na(S)
```{r}
sum(is.na(clean_data2))
```
#replace all NA values with means

```{r}
for(i in 1:ncol(clean_data2)){
      clean_data2[is.na(clean_data2[,i]), i]=mean(clean_data2[,i], na.rm=TRUE)
}
sum(is.na(clean_data2))
```

```{r}
    
for(i in 1:ncol(clean_data2)){
      if(sum(is.na(clean_data2[,i])))
        {
        print(sum(is.na(clean_data2[,i])))
        #print(names(clean_data2[i]))
        }
    }
```

#remove attributes that have no practical significance, such as calculated values, and ensure there are no NAs in the data

```{r}
remove_col=c(2,3,15,23,30,31,32,50,52,55:57,59,64,67,69,71,77,78,82,83,85,92:95,97,103,107,135:139,155,157,159,160161,164,165,167:170,177:249)
    clean_data2=clean_data2[,-remove_col]
    sum(is.na(clean_data2))
```
#number of yeses and nos in the dataset

```{r}
nos=(clean_data2[17]==2)
sum(nos)
      
yes=(clean_data2[17]==1)
sum(yes)
```
#Keep everything that isn't NA
```{r}

mydata1=clean_data2
mydf=mydata1[!is.na(mydata1[17]),]

```
#move the class attribute to the last column to get an output column.
```{r}
dim(mydf)
sum(is.na(mydf))
```
```{r}

dim(mydf)
sum(is.na(mydf))
depressive=mydf[17]
colnames(depressive)="depressive"
mydf=data.frame(mydf, depressive)
mydf=mydf[,-c(17)]
```
#printing the output column
```{r}
#Explore how many Nos and Yeses and NAs are in the dataset (remember - NAs were replaced by the mean)
      print(prop.table(table(mydf$depressive)))
```

```{r}
mydf=mydf[!(mydf$depressive=="1.82412113226952"),]
print(prop.table(table(mydf$depressive)))
```
#Subsample the dataframe 
#we have kept the ratio of yes/no similar to the larger dataset.
```{r}
set.seed(123)
mydfsmall = sample(1:nrow(mydf), size=0.1*nrow(mydf))
mentalsmall = mydf[mydfsmall,]
dim(mentalsmall)
prop.table(table(mentalsmall$depressive))

```
#SMOTE
```{r}
mentalsmall$depressive = as.factor(mentalsmall$depressive)
small_smote=SMOTE(depressive~., data=mentalsmall, k=5, perc.over=110)
table(small_smote$depressive)
levels(small_smote$depressive)=c("Yes", "No")
sum(small_smote$depressive=="Yes")
sum(small_smote$depressive=="No")
View(small_smote)
dim(small_smote)
```
#FIGURE
#compare the number of classes before and after SMOTE
```{r}
small_smote<-read.csv("C:\\Users\\Aaron Stone\\Documents\\R\\Minor 2019\\small_smote.csv")
levels(small_smote$depressive)=c("Yes", "No")
smote_graph=ggplot(small_smote, aes(depressive, fill=depressive))+geom_bar()+theme_classic(base_size = 22)+scale_fill_manual(values=c("midnightblue","firebrick4"))+guides(fill=F)+
ylab("Number of samples")+xlab("")+ggtitle("B")
        
levels(mentalsmall$depressive)=c("Yes", "No")
graph=ggplot(mentalsmall, aes(depressive,fill=depressive))+geom_bar()+theme_classic(base_size=22)+
scale_fill_manual(values=c("midnightblue", "firebrick4"))+guides(fill=F)+
ylab("Number of samples")+xlab("")+ggtitle("A")

final=grid.arrange(graph, smote_graph, nrow=1)
final
```
#Correlation
#To further clean the dataset, we are keeping attributes with only 10%+ as correlation coefficients in the final dataset.
      
#pearson's correlation to determine correlation and save the values as a CSV
```{r}
 
small_smote$depressive=as.numeric(small_smote$depressive)
write.csv(cor(small_smote, method="pearson"), "correlationoutput.csv")
write.csv(small_smote,"small_smote.csv")
```

```{r}
clean_data3<-read.csv("correlationoutput.csv")
clean_data3
```
#include only values with correlation =>0.1
```{r}
small_cor=small_smote[,c(1,4,5,7,12,15,16,19,20,21,22,23,24,26,34,35,36,37,38,39,40,75,77,129,130,131, 133)]
```
```{r}
library("ggpubr")
a<-colnames(clean_data3)
#ggscatter(clean_data3, x = a , y = "depressive", add = "", conf.int= TRUE, cor.coef = TRUE, cor.method = "pearson")
```
```{r}
#---
#Classification Algorithm
#---
    #10 fold cross validation, C4.5 (or J48) tree
        
      tc=trainControl("cv", 10, classProbs = TRUE, savePredictions = T)
      small_cor$depressive=as.factor(small_cor$depressive)
      levels(small_cor$depressive)=c("Yes", "No")
      Clean_c45_mental_tightcor=train(depressive~., data=small_cor, method="J48", trControl=tc)
      Clean_c45_mental_tightcor$results
      confusionMatrix(Clean_c45_mental_tightcor)
      Clean_c45_mental_tightcor$finalModel
      
```

```{r}
#---
#Assessing Algorithm
#---
    #Claculating the ROC area
      pred=predict(Clean_c45_mental_tightcor, newdata=small_cor, type="prob")
      
      rocgraphno=roc(predictor=pred$No, response=small_cor$depressive)
      rocgraphno
      rocgraphyes=roc(predictor=pred$Yes, response=small_cor$depressive)
      rocgraphyes$specificities=1-rocgraphyes$specificities
      rocgraphno$specificities=1-rocgraphno$specificities
      plot.roc(rocgraphyes)
      
```

```{r}
# Load rpart and rpart.plot
library(rpart)
library(rpart.plot)
small_smote<-read.csv("C:\\Users\\Aaron Stone\\Documents\\R\\Minor 2019\\small_smote.csv")
# Create a decision tree model
tree <- rpart(depressive~., data=small_smote)

# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="auto", shadow.col="gray", nn=TRUE)
```

#Random Forest

```{r}
mydata<-read.csv("C:\\Users\\Aaron Stone\\Documents\\R\\Minor 2019\\train.csv")
mydata[is.na(mydata$depressive)]<-1
ind =sample(2, nrow(mydata), replace=TRUE, prob=c(0.7, 0.3))
train.data<-mydata[ind==1,]
test.data<-mydata[ind==2,]
install.packages("randomForest")
library("randomForest")
library("class")
library("caret")
rf<-randomForest(depressive~.,data=train.data, ntree=50, proximity=T)
print(rf)
#table(predict(rf), train.data$depressive)
predict(rf, train.data$depressive)
```

#Simple Vector Machines

```{r}
small_smote2<-read.csv("small_smote.csv")
x <- subset(small_smote2, select=-depressive)
y <- depressive

#Create SVM Model and show summary

vm_tune <- tune(svm, train.x=x, train.y=y, 
              kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))

print(svm_tune)
svm_model_after_tune <- svm(depressive ~ ., data=iris, kernel="radial", cost=1, gamma=0.5)
summary(svm_model_after_tune)
pred <- predict(svm_model_after_tune,x)
system.time(predict(svm_model_after_tune,x))*100
```

#K Nearest Neighbours

```{r}
prc_train <- read.csv("C:\\Users\\Aaron Stone\\Documents\\R\\Minor 2019\\train.csv")
prc_test <- read.csv("C:\\Users\\Aaron Stone\\Documents\\R\\Minor 2019\\test.csv")
#prc_train_labels <- prc[1:65, 1]
#prc_test_labels <- prc[66:100, 1]
library(class)
prc_test_pred <- knn(train = prc_train, test = prc_test, k=10)*100
```